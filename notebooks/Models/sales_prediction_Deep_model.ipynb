{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eXGgcJyFbh-"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "pXbX_y38KxgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:  Load the data from Google Drive\n",
        "train_df = pd.read_csv('/content/drive/My Drive/rossmann-store-sales/train.csv', low_memory=False)\n",
        "test_df = pd.read_csv('/content/drive/My Drive/rossmann-store-sales/test.csv', low_memory=False)\n",
        "store_df = pd.read_csv('/content/drive/My Drive/rossmann-store-sales/store.csv', low_memory=False)"
      ],
      "metadata": {
        "id": "3CenlrsQK2kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 Merge train and test data with store data\n",
        "train_data = train_df.merge(store_df, on='Store', how='left')\n",
        "test_data = test_df.merge(store_df, on='Store', how='left')"
      ],
      "metadata": {
        "id": "bmbbf4Q8KXT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns\n",
        "train_data.drop([\"Customers\", \"PromoInterval\"], axis=1, inplace=True)\n",
        "test_data.drop([\"Id\", \"PromoInterval\"], axis=1, inplace=True)\n",
        "\n",
        "# Handle missing values\n",
        "train_data.fillna(0, inplace=True)\n",
        "test_data.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "wNdOYqvWN9dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocessing"
      ],
      "metadata": {
        "id": "EDYXAOEZOAAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 1 Extract Features from Datetime Columns\n",
        "\n",
        "def extract_date_features(df):\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['IsWeekend'] = df['Date'].dt.dayofweek >= 5\n",
        "    df['IsMonthStart'] = df['Date'].dt.is_month_start\n",
        "    df['IsMonthEnd'] = df['Date'].dt.is_month_end\n",
        "    df.drop('Date', axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "train_data = extract_date_features(train_data)\n",
        "test_data = extract_date_features(test_data)"
      ],
      "metadata": {
        "id": "_l83WLSiOAtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Encode categorical variables\n",
        "train_data['StateHoliday'] = train_data['StateHoliday'].replace({'0': 0, 'a': 1, 'b': 2, 'c': 3}).astype(int)\n",
        "train_data['StoreType'] = train_data['StoreType'].map({'a': 1, 'b': 2, 'c': 3, 'd': 4})\n",
        "train_data['Assortment'] = train_data['Assortment'].map({'a': 1, 'b': 2, 'c': 3})\n",
        "\n",
        "test_data['StateHoliday'] = test_data['StateHoliday'].replace({'0': 0, 'a': 1, 'b': 2, 'c': 3}).astype(int)\n",
        "test_data['StoreType'] = test_data['StoreType'].map({'a': 1, 'b': 2, 'c': 3, 'd': 4})\n",
        "test_data['Assortment'] = test_data['Assortment'].map({'a': 1, 'b': 2, 'c': 3})\n"
      ],
      "metadata": {
        "id": "pt-6h_42OBBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 Scale the Data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_features = train_data.drop('Sales', axis=1)\n",
        "train_labels = train_data['Sales']\n",
        "scaled_train_features = scaler.fit_transform(train_features)\n",
        "\n",
        "test_features = test_data\n",
        "scaled_test_features = scaler.transform(test_features)\n",
        "\n",
        "# Convert scaled features back to DataFrame for convenience\n",
        "scaled_train_df = pd.DataFrame(scaled_train_features, columns=train_features.columns)\n",
        "scaled_test_df = pd.DataFrame(scaled_test_features, columns=test_features.columns)"
      ],
      "metadata": {
        "id": "n7RK232WOB3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Building Models with Sklearn Pipelines"
      ],
      "metadata": {
        "id": "Yp5dSjrlOZ3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1  Define and Train the Model by Using RandomForrestRegressor within Sklearn Pipelines\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(scaled_train_df, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Validate the model\n",
        "y_pred = pipeline.predict(X_val)\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print(f\"Validation MSE: {mse}\")\n",
        "\n",
        "# Train the model on the full training set\n",
        "pipeline.fit(scaled_train_df, train_labels)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = pipeline.predict(scaled_test_df)"
      ],
      "metadata": {
        "id": "HSgomGixOZ-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 Choose a Loss Function\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "print(f'Mean Absolute Error: {mae}')"
      ],
      "metadata": {
        "id": "Q63UVi9SOaIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Post Prediction Analysis"
      ],
      "metadata": {
        "id": "WaWyrxYROaMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 Feature Importance\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "importances = pipeline.named_steps['model'].feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Feature': train_features.columns, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
        "plt.title('Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6K1p6FYXO0Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2 Confidence Interval Estimation\n",
        "\n",
        "# Estimating confidence intervals using bootstrapping\n",
        "# Number of bootstrap samples\n",
        "from sklearn.utils import resample\n",
        "\n",
        "n_bootstraps = 1\n",
        "bootstrap_preds = np.zeros((n_bootstraps, len(X_val)))\n",
        "\n",
        "# Generate bootstrap samples, train, and predict\n",
        "for i in range(n_bootstraps):\n",
        "    X_train_bootstrap, y_train_bootstrap = resample(X_train, y_train, random_state=i)\n",
        "    pipeline.fit(X_train_bootstrap, y_train_bootstrap)\n",
        "    bootstrap_preds[i] = pipeline.predict(X_val)\n",
        "\n",
        "# Calculate the confidence intervals\n",
        "lower_percentile = 2.5\n",
        "upper_percentile = 97.5\n",
        "lower_bound = np.percentile(bootstrap_preds, lower_percentile, axis=0)\n",
        "upper_bound = np.percentile(bootstrap_preds, upper_percentile, axis=0)\n",
        "\n",
        "# Calculate the mean predictions\n",
        "y_pred_mean = np.mean(bootstrap_preds, axis=0)\n",
        "\n",
        "# Display the results\n",
        "results_df = pd.DataFrame({\n",
        "    'Actual': y_val,\n",
        "    'Predicted Mean': y_pred_mean,\n",
        "    'Lower Bound': lower_bound,\n",
        "    'Upper Bound': upper_bound\n",
        "})\n",
        "\n",
        "print(results_df.head())"
      ],
      "metadata": {
        "id": "6ilS-suNO6MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Serialize Models\n",
        "\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the current timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "\n",
        "# Define the file path\n",
        "model_filename = f'model_{timestamp}.pkl'\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(pipeline, model_filename)\n",
        "\n",
        "print(f'Model saved as {model_filename}')"
      ],
      "metadata": {
        "id": "vlotbIjSXPkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Building a Deep Learning Model with LSTM deep leatning model"
      ],
      "metadata": {
        "id": "y8l7kTWxnCD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 Isolate the Time Series Data\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select relevant columns\n",
        "time_series_df = train_data[['Date', 'Sales']].sort_values(by='Date')\n",
        "\n",
        "# Set the Date column as the index\n",
        "time_series_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Check if the time series is stationary\n",
        "def check_stationarity(series):\n",
        "    from statsmodels.tsa.stattools import adfuller\n",
        "    result = adfuller(series)\n",
        "    print('ADF Statistic:', result[0])\n",
        "    print('p-value:', result[1])\n",
        "    return result[1] <= 0.05\n",
        "\n",
        "# Check stationarity\n",
        "is_stationary = check_stationarity(time_series_df['Sales'])\n",
        "\n",
        "# Difference the data if not stationary\n",
        "if not is_stationary:\n",
        "    time_series_df['Sales'] = time_series_df['Sales'].diff().dropna()\n",
        "\n",
        "# Check for autocorrelation and partial autocorrelation\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "plot_acf(time_series_df['Sales'])\n",
        "plot_pacf(time_series_df['Sales'])\n",
        "plt.show()\n",
        "\n",
        "# Scale data between -1 and 1\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaled_data = scaler.fit_transform(time_series_df['Sales'].values.reshape(-1, 1))\n",
        "\n",
        "# Create supervised learning data\n",
        "def create_dataset(data, time_step=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_step - 1):\n",
        "        a = data[i:(i + time_step), 0]\n",
        "        X.append(a)\n",
        "        y.append(data[i + time_step, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "time_step = 10\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "\n",
        "# Reshape input to [samples, time steps, features] for LSTM\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)"
      ],
      "metadata": {
        "id": "a4EbrOdZnEWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2 Build and Train the LSTM Model\n",
        "\n",
        "# Define the LSTM model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
        "model.add(tf.keras.layers.LSTM(50))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=20, batch_size=1, verbose=2)\n",
        "\n",
        "# Save the model\n",
        "model_filename = f'lstm_model-{timestamp}.h5'\n",
        "model.save(model_filename)\n",
        "print(f'LSTM model saved as {model_filename}')"
      ],
      "metadata": {
        "id": "Cyb4oksNnEfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Making Predictions\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model(model_filename)\n",
        "\n",
        "# Prepare data for prediction (e.g., last `time_step` days)\n",
        "last_data = scaled_data[-time_step:].reshape(1, time_step, 1)\n",
        "\n",
        "# Make a prediction\n",
        "predicted_sales = loaded_model.predict(last_data)\n",
        "\n",
        "# Inverse transform the prediction\n",
        "predicted_sales = scaler.inverse_transform(predicted_sales)\n",
        "print(f'Predicted Sales: {predicted_sales[0][0]}')"
      ],
      "metadata": {
        "id": "4S6PU8ganJTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}